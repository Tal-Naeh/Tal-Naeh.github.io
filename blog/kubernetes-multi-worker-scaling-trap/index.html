<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-ECGK9Y42TF"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-ECGK9Y42TF');</script><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Tal's Blog" href="https://tal-naeh.github.io/rss.xml"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><!-- Primary Meta Tags --><title>The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability</title><meta name="title" content="The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability"><meta name="description" content="How we discovered our 'healthy' pods were silently dropping 25% of requests — and why 1 worker per pod might be the answer you didn't know you needed."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><meta property="og:title" content="The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability"><meta property="og:description" content="How we discovered our 'healthy' pods were silently dropping 25% of requests — and why 1 worker per pod might be the answer you didn't know you needed."><meta property="og:image" content="https://logo.svgcdn.com/l/kubernetes.png"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><meta property="twitter:title" content="The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability"><meta property="twitter:description" content="How we discovered our 'healthy' pods were silently dropping 25% of requests — and why 1 worker per pod might be the answer you didn't know you needed."><meta property="twitter:image" content="https://logo.svgcdn.com/l/kubernetes.png"><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:0}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:720px;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>Tal's Blog</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/about" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> About </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://linkedin.com/in/tal-naeh" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Follow on LinkedIn</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://github.com/Tal-Naeh" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Visit my GitHub</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="hero-image" data-astro-cid-bvzihdzo> <img src="https://logo.svgcdn.com/l/kubernetes.png" alt="The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability" style="max-width: 400px; height: auto;" data-astro-cid-bvzihdzo> </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2025-07-12T00:00:00.000Z"> Jul 12, 2025 </time> <div class="last-updated-on" data-astro-cid-bvzihdzo>
Last updated on <time datetime="2025-07-12T00:00:00.000Z"> Jul 12, 2025 </time> </div> </div> <h1 data-astro-cid-bvzihdzo>The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability</h1> <hr data-astro-cid-bvzihdzo> </div>  

<p><em>How we discovered our "healthy" pods were silently dropping 25% of requests — and why 1 worker per pod might be the answer you didn't know you needed.</em></p>

<hr>

<p>When our monitoring showed green lights across the board but customers complained about random timeouts, we thought we had a network issue. Turns out, we had stumbled into one of Kubernetes' most deceptive anti-patterns: the multi-worker pod trap.</p>

<h2 id="the-false-paradise-of-multi-worker-pods">The False Paradise of Multi-Worker Pods</h2>

<p>Picture this: you're running FastAPI with uvicorn, and naturally, you configure it the "high-performance" way:</p>

<pre><code class="language-bash">uvicorn main:app --workers 4
</code></pre>

<p>One pod, four workers. Efficient, right? Your Kubernetes dashboard shows:</p>
<ul>
<li>✅ Pod Status: Running</li>
<li>✅ Health Check: 200 OK</li>
<li>✅ Memory/CPU: Normal ranges</li>
<li>✅ No restarts or crashes</li>
</ul>

<p>But lurking beneath this green paradise is a silent killer.</p>

<h2 id="the-black-hole-problem">The "Black Hole" Problem</h2>

<p>Here's what actually happens when one worker gets stuck in an infinite loop or deadlocks:</p>

<pre><code>┌─────────────────┐
│   Pod (Running) │
│                 │
│ Worker 1: ✅    │  ← Handling requests fine
│ Worker 2: 💀    │  ← Stuck in infinite loop
│ Worker 3: ✅    │  ← Handling requests fine  
│ Worker 4: ✅    │  ← Handling requests fine
└─────────────────┘
</code></pre>

<p><strong>The terrifying result:</strong> 25% of your requests vanish into a black hole. No errors, no alerts, no visibility. Just silent failures that make your users think your API is flaky.</p>

<p>We discovered this the hard way when investigating why our 99.9% uptime SLA was being violated despite "perfect" monitoring metrics.</p>

<h2 id="why-traditional-monitoring-fails-here">Why Traditional Monitoring Fails Here</h2>

<p>Kubernetes health checks are pod-level, not worker-level. When you have multiple workers:</p>

<ul>
<li><strong>Health probe hits any working process</strong> → Returns 200 OK</li>
<li><strong>Load balancer routes randomly</strong> → Some requests hit the broken worker</li>
<li><strong>Logs are mixed</strong> → Hard to trace which worker caused issues</li>
<li><strong>Resource metrics are aggregated</strong> → No per-worker visibility</li>
</ul>

<p>It's like having a restaurant where one chef is unconscious, but the health inspector only checks if the kitchen lights are on.</p>

<h2 id="the-counter-intuitive-solution-less-is-more">The Counter-Intuitive Solution: Less is More</h2>

<p>After extensive testing and analysis, we arrived at a surprising conclusion: <strong>1 worker per pod + Horizontal Pod Autoscaler (HPA) is almost always better</strong>.</p>

<h3 id="before-1-pod-4-workers">Before: 1 Pod × 4 Workers</h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: app
        command: ["uvicorn", "main:app", "--workers", "4"]
</code></pre>

<h3 id="after-4-pods-1-worker">After: 4 Pods × 1 Worker</h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 4
  template:
    spec:
      containers:
      - name: app
        command: ["uvicorn", "main:app"]  # No --workers flag
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: my-app
  minReplicas: 4
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
</code></pre>

<h2 id="the-benefits-we-didnt-expect">The Benefits We Didn't Expect</h2>

<h3 id="1-crystal-clear-failure-detection">1. <strong>Crystal Clear Failure Detection</strong></h3>
<ul>
<li>Worker crash = Pod crash = Immediate restart</li>
<li>No more silent failures</li>
<li>Clean, traceable logs per instance</li>
</ul>

<h3 id="2-granular-scaling">2. <strong>Granular Scaling</strong></h3>
<p>Instead of scaling by 4× chunks (1 pod → 8 workers), you can scale by single units (4 pods → 5 pods).</p>

<h3 id="3-better-fault-isolation">3. <strong>Better Fault Isolation</strong></h3>
<p>When one instance fails, only 1/N of traffic is affected, not 1/workers-per-pod.</p>

<h3 id="4-simpler-debugging">4. <strong>Simpler Debugging</strong></h3>
<pre><code class="language-bash"># Before: Which worker caused this error?
kubectl logs my-app-pod  # Mixed logs from 4 workers

# After: Clear ownership
kubectl logs my-app-abc123  # Logs from one worker only
</code></pre>

<h2 id="the-ml-model-exception">The ML Model Exception</h2>

<p>There's one significant exception to this rule: <strong>machine learning services with heavy models</strong>.</p>

<p>Consider a model that takes 2 minutes to load and uses 4GB of RAM:</p>

<pre><code class="language-python">class ModelService:
    def __init__(self):
        # This takes 120 seconds and 4GB RAM
        self.model = load_huge_transformer_model()
        self.preprocessor = load_preprocessing_pipeline()
</code></pre>

<p>With 1 worker per pod:</p>
<ul>
<li><strong>Pod restart</strong> = 2 minutes of complete unavailability</li>
<li><strong>Scale up</strong> = 2 minutes before new capacity is ready</li>
<li><strong>Cost</strong> = High resource overhead per replica</li>
</ul>

<p>In this case, multi-worker pods make sense, but with crucial modifications:</p>

<pre><code class="language-yaml"># For heavy ML services only
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 2  # Fewer pods
  template:
    spec:
      containers:
      - name: ml-service
        command: ["uvicorn", "main:app", "--workers", "3"]
        resources:
          requests:
            memory: "6Gi"  # Accommodate model + workers
            cpu: "2000m"
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 180  # Wait for model loading
          timeoutSeconds: 30
        # Add worker-specific health monitoring
        livenessProbe:
          httpGet:
            path: /health/workers  # Custom endpoint
            port: 8000
          initialDelaySeconds: 240
</code></pre>

<p>Plus a custom health endpoint that actually checks all workers:</p>

<pre><code class="language-python">@app.get("/health/workers")
async def worker_health():
    """Check that all workers can perform inference"""
    try:
        # Test inference on all workers
        test_result = await model.predict_async(test_input)
        worker_count = get_active_worker_count()
        return {
            "status": "healthy",
            "workers": worker_count,
            "model_loaded": True
        }
    except Exception as e:
        raise HTTPException(503, f"Worker health check failed: {e}")
</code></pre>

<h2 id="implementation-strategy">Implementation Strategy</h2>

<h3 id="phase-1-assess-your-services">Phase 1: Assess Your Services</h3>
<p>Categorize your services:</p>

<p><strong>Standard APIs (95% of services):</strong></p>
<ul>
<li>FastAPI CRUD operations</li>
<li>API gateways</li>
<li>Lightweight processing</li>
<li><strong>Recommendation:</strong> 1 worker per pod</li>
</ul>

<p><strong>Heavy Model Services:</strong></p>
<ul>
<li>ML inference with models > 1GB</li>
<li>Startup time > 30 seconds</li>
<li><strong>Recommendation:</strong> Consider multi-worker with enhanced monitoring</li>
</ul>

<h3 id="phase-2-pilot-implementation">Phase 2: Pilot Implementation</h3>
<p>Start with a non-critical service:</p>

<pre><code class="language-bash"># 1. Add resource limits
kubectl patch deployment pilot-service -p '{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "app",
          "resources": {
            "requests": {"memory": "128Mi", "cpu": "100m"},
            "limits": {"memory": "256Mi", "cpu": "200m"}
          }
        }]
      }
    }
  }
}'

# 2. Remove --workers flag and scale replicas
# 3. Add HPA
# 4. Monitor for a week
</code></pre>

<h3 id="phase-3-monitor-the-right-metrics">Phase 3: Monitor the Right Metrics</h3>

<p><strong>Before Implementation:</strong></p>
<ul>
<li>Pod restart rate</li>
<li>Request error rate (may be underreported)</li>
<li>Response time percentiles</li>
<li>Resource utilization</li>
</ul>

<p><strong>After Implementation:</strong></p>
<ul>
<li>Instance failure rate (should be more visible)</li>
<li>HPA scaling events</li>
<li>Resource efficiency</li>
<li>True error rates (should be more accurate)</li>
</ul>

<h2 id="the-resource-management-foundation">The Resource Management Foundation</h2>

<p>None of this works without proper resource management. You need:</p>

<h3 id="1-resource-requests-and-limits">1. Resource Requests and Limits</h3>
<pre><code class="language-yaml">resources:
  requests:     # What Kubernetes reserves for you
    memory: "128Mi"
    cpu: "100m"
  limits:       # Maximum you can use
    memory: "256Mi"  
    cpu: "200m"
</code></pre>

<h3 id="2-horizontal-pod-autoscaler">2. Horizontal Pod Autoscaler</h3>
<pre><code class="language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
</code></pre>

<h3 id="3-cluster-autoscaler">3. Cluster Autoscaler</h3>
<p>Ensure your cloud provider can add nodes when needed:</p>

<pre><code class="language-bash"># EKS example
eksctl create nodegroup \
  --cluster=my-cluster \
  --nodes-min=2 \
  --nodes-max=10 \
  --node-type=m5.large
</code></pre>

<h2 id="our-results">Our Results</h2>

<p>Six months after implementing this strategy:</p>

<p><strong>Reliability Improvements:</strong></p>
<ul>
<li>📈 <strong>MTTR reduced by 60%</strong> (faster problem detection)</li>
<li>📈 <strong>Alert accuracy up 90%</strong> (no more false positives)</li>
<li>📈 <strong>Customer-reported errors down 45%</strong> (better fault isolation)</li>
</ul>

<p><strong>Operational Benefits:</strong></p>
<ul>
<li>📈 <strong>Debugging time cut in half</strong> (cleaner logs, clearer ownership)</li>
<li>📈 <strong>Infrastructure costs down 30%</strong> (during off-peak hours via auto-scaling)</li>
<li>📈 <strong>Team velocity increased</strong> (less time firefighting mysterious issues)</li>
</ul>

<h2 id="the-bottom-line">The Bottom Line</h2>

<p>The multi-worker pod pattern feels efficient, but it creates invisible reliability problems that traditional monitoring can't catch. For most applications, the simplicity and clarity of 1 worker per pod + HPA far outweighs any theoretical performance benefits.</p>

<p><strong>Exception:</strong> Heavy ML models where startup cost truly justifies the complexity—but even then, you need enhanced monitoring to avoid the black hole problem.</p>

<p><strong>Action items:</strong></p>
<ol>
<li>Audit your current worker configuration</li>
<li>Identify your "black hole" risk services</li>
<li>Start with a pilot migration</li>
<li>Implement proper resource limits and HPA</li>
<li>Monitor the improvement in true error visibility</li>
</ol>

<p>Your future self—and your users—will thank you for choosing reliability over premature optimization.</p>

<hr>


 </div> </article> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 Tal Naeh. All rights reserved.
<div class="social-links" data-astro-cid-sz7xmlte> <a href="https://linkedin.com/in/tal-naeh" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Follow on LinkedIn</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/twitter" data-astro-cid-sz7xmlte><path fill="currentColor" d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z" data-astro-cid-sz7xmlte></path></svg> </a> <a href="https://github.com/Tal-Naeh" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Visit my GitHub</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/github" data-astro-cid-sz7xmlte><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-sz7xmlte></path></svg> </a> </div> </footer>  </body></html>