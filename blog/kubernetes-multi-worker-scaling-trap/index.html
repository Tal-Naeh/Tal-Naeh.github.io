<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-ECGK9Y42TF"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-ECGK9Y42TF');</script><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/png" href="/DOL.png"><!-- Security Headers --><meta http-equiv="X-Frame-Options" content="DENY"><meta http-equiv="X-Content-Type-Options" content="nosniff"><meta http-equiv="Referrer-Policy" content="strict-origin-when-cross-origin"><meta http-equiv="Permissions-Policy" content="geolocation=(), microphone=(), camera=()"><meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https://www.googletagmanager.com https://www.google-analytics.com; style-src 'self' 'unsafe-inline'; img-src 'self' data: https://media.licdn.com https://www.jenkins.io https://raw.githubusercontent.com https://logo.svgcdn.com https://www.docker.com https://git-scm.com; font-src 'self'; connect-src 'self' https://www.google-analytics.com; frame-ancestors 'none'"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="DevOps Out Loud" href="https://tal-naeh.github.io/rss.xml"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><!-- Primary Meta Tags --><title>Why We Switched to 1 Worker Per Pod in Kubernetes</title><meta name="title" content="Why We Switched to 1 Worker Per Pod in Kubernetes"><meta name="description" content="How multi-worker pods were hiding failures from our monitoring ‚Äî and why single-worker pods made debugging 10x easier"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><meta property="og:title" content="Why We Switched to 1 Worker Per Pod in Kubernetes"><meta property="og:description" content="How multi-worker pods were hiding failures from our monitoring ‚Äî and why single-worker pods made debugging 10x easier"><meta property="og:image" content="https://logo.svgcdn.com/l/kubernetes.png"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://tal-naeh.github.io/blog/kubernetes-multi-worker-scaling-trap/"><meta property="twitter:title" content="Why We Switched to 1 Worker Per Pod in Kubernetes"><meta property="twitter:description" content="How multi-worker pods were hiding failures from our monitoring ‚Äî and why single-worker pods made debugging 10x easier"><meta property="twitter:image" content="https://logo.svgcdn.com/l/kubernetes.png"><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:0}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:720px;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>DevOps Out Loud</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/about" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> About </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://linkedin.com/in/tal-naeh" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Follow on LinkedIn</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://github.com/Tal-Naeh" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Visit my GitHub</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="hero-image" data-astro-cid-bvzihdzo> <img src="https://logo.svgcdn.com/l/kubernetes.png" alt="The Hidden Scaling Trap: Why Your Kubernetes Multi-Worker Setup is Sabotaging Your Reliability" style="max-width: 400px; height: auto;" data-astro-cid-bvzihdzo> </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2025-01-12T00:00:00.000Z"> Jan 12, 2025 </time> <div class="last-updated-on" data-astro-cid-bvzihdzo>
Last updated on <time datetime="2025-01-12T00:00:00.000Z"> Jan 12, 2025 </time> </div> </div> <h1 data-astro-cid-bvzihdzo>Why We Switched to 1 Worker Per Pod in Kubernetes</h1> <hr data-astro-cid-bvzihdzo> </div>

<p><strong>Our "healthy" pods were hiding failures from Kubernetes, and we had no visibility into what was actually breaking. Here's why we switched to single-worker pods‚Äîand why you might want to consider it too.</strong></p>

<p>When our monitoring showed green lights but customers reported intermittent timeouts, we thought we had a network issue. After weeks of debugging, we found the culprit: multiple workers per pod were hiding failures from Kubernetes' health checks.</p>

<p>This isn't a universal truth‚Äîplenty of teams successfully run multi-worker pods. But if you're struggling with mysterious failures, hard-to-debug issues, or applications with worker-level bugs, this might help.</p>

<h2 id="the-visibility-problem">The Visibility Problem</h2>

<p>When users reported "the API is slow sometimes" and we checked our dashboards, everything looked healthy. All green lights, normal metrics.</p>

<p>This went on for months.</p>

<p>We had the classic FastAPI setup that everyone uses:</p>

<pre><code class="language-python">uvicorn main:app --workers 4</code></pre>

<p>Makes sense, right? One pod, four workers. More workers = more throughput. It's what the docs suggest, what Stack Overflow recommends, what we've all been doing since forever.</p>

<p>Except it was randomly eating requests, and we had no idea.</p>

<h2 id="the-debugging-journey">The Debugging Journey</h2>

<p>It started with sporadic customer complaints. Not many, but enough to investigate. The weird part? We could never reproduce it. Our logs showed the requests coming in, but then... nothing. No error, no timeout logged, just silence.</p>

<p>After adding way too much instrumentation, we finally caught it:</p>

<pre><code>Pod Status: ‚úÖ Running
Health Check: ‚úÖ 200 OK
Reality: Worker 2 had been deadlocked for 47 minutes</code></pre>

<p>The pod was "healthy" because the health check hit Worker 1. Meanwhile, Worker 2 was stuck in an infinite loop, silently dropping requests that landed on it.</p>

<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pod (Running)   ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ Worker 1: ‚úÖ    ‚îÇ ‚Üê Health check always hits this one
‚îÇ Worker 2: üíÄ    ‚îÇ ‚Üê Been dead for an hour
‚îÇ Worker 3: ‚úÖ    ‚îÇ 
‚îÇ Worker 4: ‚úÖ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>

<h2 id="why-this-is-hard-to-detect">Why This is Hard to Detect</h2>

<p>The challenging part is that standard monitoring often won't catch this.</p>

<p>Kubernetes checks if the pod is healthy, not individual workers. Your health endpoint returns 200 OK because <em>one</em> worker responds. Your CPU and memory look normal because three workers are fine. Your logs are a mess because all four workers write to the same stdout.</p>

<p>It's like having a 4-person customer service team where one person just went home but nobody noticed because the phone still gets answered... eventually.</p>

<h2 id="the-core-issue">The Core Issue</h2>

<p>After diving deep into this, we realized something that sounds obvious in hindsight: <strong>Kubernetes manages pods, not processes within pods. When you run multiple workers, Kubernetes can't see if individual workers fail.</strong></p>

<p>It's like hiring a manager and then not telling them about three quarters of your team. They can't manage what they don't know exists.</p>

<h2 id="the-solution-that-felt-wrong-but-worked">The Solution That Felt Wrong (But Worked)</h2>

<p>We switched to 1 worker per pod.</p>

<p>Yes, really.</p>

<pre><code class="language-yaml"># Before: 1 pod with 4 workers
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: app
        command: ["uvicorn", "main:app", "--workers", "4"]

# After: 4 pods with 1 worker each
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 4
  template:
    spec:
      containers:
      - name: app
        command: ["uvicorn", "main:app"]  # No --workers flag
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"</code></pre>

<p>At first, this felt like going backwards. More pods? Isn't that wasteful?</p>

<p>We also added an HPA:</p>

<pre><code class="language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: my-app
  minReplicas: 4
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70</code></pre>

<p>This combination gave us several benefits.</p>

<h2 id="what-actually-improved">What Actually Improved</h2>

<p><strong>1. Failures became visible</strong></p>

<p>Worker crash = Pod crash = Kubernetes restarts it immediately. No more silent failures.</p>

<p><strong>2. Debugging became possible</strong></p>

<pre><code class="language-bash"># Before: Which worker logged this error??
kubectl logs my-app-pod-abc123
[ERROR] Database connection failed
[INFO] Request processed
[ERROR] Database connection failed
[INFO] Request processed
# All workers mixed together üò≠

# After: Clear logs per pod
kubectl logs my-app-pod-abc123
[ERROR] Database connection failed
# Ah, THIS specific instance has DB issues</code></pre>

<p><strong>3. Scaling became granular</strong></p>

<p>Instead of jumping from 4 workers to 8 (doubling capacity), we could go from 4 to 5 pods. Much smoother.</p>

<p><strong>4. Resource limits actually worked</strong></p>

<p>Before, one worker could hog CPU and starve the others. Now, Kubernetes enforces limits per pod. Fair and predictable.</p>

<h2 id="but-what-about">But What About...</h2>

<h3 id="isnt-this-more-overhead">"Isn't this more overhead?"</h3>

<p>Yes. 4 pods use more total memory than 1 pod with 4 workers (no Copy-on-Write sharing between pods). However, the isolation is valuable. Each pod gets its own resources, no workers fighting over CPU, and when something breaks, you know exactly which instance is the problem. The trade-off in memory usage is often worth the improved debugging experience.</p>

<h3 id="what-about-startup-time">"What about startup time?"</h3>

<p>Fair point. If your app takes 30 seconds to start, this might be annoying during deploys. But here's the thing: with rolling deployments and proper readiness probes, users never notice.</p>

<h3 id="what-about-shared-resources">"What about shared resources?"</h3>

<p>This is the one real trade-off. If you're loading a 2GB ML model, you probably don't want 10 pods each loading their own copy. More on this below.</p>

<h2 id="the-ml-exception-and-why-it-proves-the-rule">The ML Exception (And Why It Proves The Rule)</h2>

<p>There's one case where we kept multi-worker pods: our ML inference service.</p>

<p>The model takes 2 minutes to load and uses 4GB of RAM. Running 10 pods would mean 40GB of RAM just for model copies‚Äîan inefficient use of resources.</p>

<p>So for that service:</p>

<pre><code class="language-python"># ML service keeps multiple workers
uvicorn ml_service:app --workers 3

# But we added aggressive health checking
@app.get("/health/workers")
async def check_all_workers():
    """Actually verify all workers can process requests"""
    results = []
    for worker_id in range(3):
        try:
            # Each worker must prove it's alive
            test_inference = await run_inference_test(worker_id)
            results.append({"worker": worker_id, "status": "ok"})
        except:
            results.append({"worker": worker_id, "status": "dead"})
    
    if any(r["status"] == "dead" for r in results):
        raise HTTPException(503, detail=results)
    
    return results</code></pre>

<p>The key: if you must use multiple workers, at least monitor them properly.</p>

<h2 id="how-to-know-if-you-have-this-problem">How To Know If You Have This Problem</h2>

<p>To verify if this affects your setup:</p>

<ol>
<li>Find a multi-worker pod</li>
<li>Exec into it and kill -STOP one worker process</li>
<li>Observe that health checks still pass</li>
<li>Watch some requests timeout</li>
<li>Consider if this visibility gap is acceptable</li>
</ol>

<p>If you're using <code>--workers</code> in your deployments, you may be affected by this issue.</p>

<h2 id="migration-strategy-the-realistic-version">Migration Strategy (The Realistic Version)</h2>

<p>Don't big-bang this. Here's what worked for us:</p>

<p><strong>Week 1: Pick your least important service</strong></p>
<ul>
<li>That internal admin tool with 5 users? Perfect.</li>
<li>Remove --workers, increase replicas</li>
<li>Watch it for a week</li>
</ul>

<p><strong>Week 2-3: Move to a real service</strong></p>
<ul>
<li>Pick something customer-facing but not critical</li>
<li>Add good resource limits</li>
<li>Set up an HPA</li>
<li>Monitor error rates obsessively</li>
</ul>

<p><strong>Week 4+: Gradual rollout</strong></p>
<ul>
<li>Service by service</li>
<li>Keep a list of "exceptions" (ML services, etc.)</li>
<li>Document why each exception exists</li>
</ul>

<h2 id="what-changed">What Changed</h2>

<p>The migration to single-worker pods made the most difference in operational visibility and debugging workflow.</p>

<p><strong>What improved:</strong></p>
<ul>
<li>Failed workers now trigger pod restarts immediately</li>
<li>Logs became easier to trace to specific instances</li>
<li>Debugging time decreased noticeably</li>
<li>The mysterious timeout issues mostly disappeared</li>
</ul>

<p><strong>What stayed the same:</strong></p>
<ul>
<li>Overall application performance and latency</li>
<li>Total resource usage (with proper limits configured)</li>
</ul>

<p><strong>What required adjustment:</strong></p>
<ul>
<li>Resource limits needed recalculation per pod</li>
<li>Deployment configurations across all services</li>
<li>Team's mental model of how pods scale</li>
</ul>

<h2 id="what-this-taught-us">What This Taught Us</h2>

<p>The bigger lesson here isn't about workers or pods. It's that <strong>sometimes the "best practice" from the VM era doesn't translate to Kubernetes</strong>.</p>

<p>We've been trained to think "fewer processes = better" because managing processes used to be hard. But Kubernetes is really good at managing lots of pods. That's literally its job.</p>

<p>Let it do its job.</p>

<h2 id="the-tldr">The TL;DR</h2>

<ul>
<li>Multiple workers per pod can lead to silent failures and difficult debugging</li>
<li>1 worker per pod + HPA provides visible failures and clearer debugging</li>
<li>Exception: Services with high startup costs (ML models)</li>
<li>Migration: Start small, measure everything</li>
<li>The improved visibility pays off during production debugging</li>
</ul>

<p>I know this approach might seem like extra work initially. We had the same hesitation. But after experiencing the operational improvements, we found that in Kubernetes, visibility and clear failure modes often matter more than minimal resource usage.</p>

<p>Consider trying it with one non-critical service first. You can always revert if it doesn't work for your use case.</p>

<hr>

<p><em>Got war stories about multi-worker pods? Found a case where they actually make sense? Hit me up. I'm genuinely curious about other experiences with this pattern.</em></p>

 </div> </article> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 Tal Naeh. All rights reserved.
<div class="social-links" data-astro-cid-sz7xmlte> <a href="https://linkedin.com/in/tal-naeh" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Follow on LinkedIn</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/twitter" data-astro-cid-sz7xmlte><path fill="currentColor" d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173